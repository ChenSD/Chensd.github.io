---
layout: post
title:  自然语言处理（NLP）
date:   2023-3-18 9:10:20
categories: Emotion
---

# 常识

* 文本情感计算的过程可以由 3 部分组成：文本信息采集、文本情感特征提取和文本情感信息分类。

* 文本情感分析主要有三大任务，即文本情感特征提取、文本情感特征分类以及文本情感特征检索与归纳。而关于文本情感分析的方法主要分为两类：
  * 基于情感词典的方法
    * 人工构建情感词典
    * 自动构建情感词典
  * 基于机器学习的方法
    * 朴素贝叶斯
    * 最大熵
    * SVM分类器

* Pyhthon的spaCy包，可以把自然语言，分解为包含不同成分的语言库（doc）。
* token为NLP分析的基本单位，包括单词、标点符号等。
* 在spaCy中，'tagger'，'parser'，和'ner' 是指在自然语言处理 (NLP) 管道中的三个关键组件。它们分别负责不同的任务，以下是它们的简要介绍：
    * 'tagger'：词性标注器 (Part-of-Speech Tagger)，也简称为POS标记
      'tagger'负责为文本中的每个单词分配词性标签。词性标注是NLP中的一种基本任务，它为后续的任务（如依存关系解析和命名实体识别）提供了有用的信息。词性包括名词、动词、形容词、副词等，它们有助于了解单词在句子中的功能和语法关系。
    * 'parser'：句法解析器 (Dependency Parser)
      'parser'负责识别文本中的单词之间的句法关系。句法解析是一种将句子表示为依存关系树的过程，其中树的节点表示单词，边表示单词之间的依存关系。这有助于识别句子中的主题、宾语、修饰语等成分，从而理解句子的结构和意义。其中，依存标记ROOT表示句子中的主要动词或动作。
    * 'ner'：命名实体识别器 (Named Entity Recognizer)
      'ner'负责从文本中检测和分类命名实体，如人名、地名、组织名、日期等。命名实体识别是信息提取和文本分析的重要组成部分，因为它可以帮助我们识别文本中的关键实体及其关系。这对于许多应用程序（如情感分析、知识图谱构建和实体链接）来说是非常有价值的。
* 在spaCy中，这些组件通常按照顺序（'tagger' -> 'parser' -> 'ner'）应用于文本，因为后续组件通常依赖于前面组件生成的信息。

# 基于规则的中文命名-实体识别

* 逻辑：自己制定规则，然后利用自定义规则去识别文本内容。这是一种先验的方法。其局限主要在于规则制定者的经验和水平。如果其是一名对某规则了解透彻的专家，则其制定的规则就可能非常可靠，反之则不确定其可靠性。
* spaCy对中文的实体识别不佳，反正我目前测试是这样
* 经查询，如果想要实现符合自己目标的中文识别，需要用到名为EntityRuler的spaCy管道组件。该组件可以通过基于patterns字典添加“命名实体”，从而实现基于规则和统计的方式来识别和命名实体。

# 基于神经网络的中文句子识别

* 逻辑：首先人工标注一些数据，把数据分为训练集和测试集。训练集用于训练分类器，测试集用来测试分类器。
* 使用的算法：长短期记忆递归神经网络（Long Short-Term Memory，LSTM）最早由 Sepp Hochreiter和Jürgen Schmidhuber于1997年提出，是一种特定形式的RNN（Recurrent neural network，循环神经网络）。本研究使用双向LSTM算法来进行文本识别。
* 基于Python的句法分析与正则表达式结合标注句子类型：先使用Spacy进行句法分析，然后根据句子结构的属性，构造正则表达式；最后，将正则表达式与句法分析结合对疑问句，陈述句和特殊句进行句子类型标注。该方法适合制作小型的训练库。


# 实践

## 准备工作

* 经验教训1：电脑上装了多个版本的Python，导致不同的包安装环境混乱。Spyder运行时，一直找不到某个包。正确的顺序应该是先选择合适的Python版本，将其配置到环境路径中，再进行各种包的安装。
* 经验教训2：spacy，numpy，TensorFlow，Python，四个玩意之间不同的版本不兼容，幸好是都标注了所支持版本。这个需要注意，版本不对应装不上。
* 中文对话语料库合集：https://github.com/codemayq/chinese_chatbot_corpus，数据量至少在1000以上，1W以上最常见。
* 训练结果：
  loss：训练集损失值
  accuracy:训练集准确率
  val_loss:测试集损失值
  val_accruacy:测试集准确率（关键指标）

## 基本流程

1. 文本清洗。去除文本中的特殊字符，例如句首的数字编号，1、2、3等；文本中的图片、视频等文字无关内容。
2. 进行NLP分析。分句、分词、甚至分字。然后得到每个对象的编号（IDs）、属性（如名词还是动词）、和词义，以及不同对象之间的依赖关系（主谓宾，还是主系表）。  

# 常用命令

* 切换回默认源命令为 pip config unset global.index-url。
* 常用工具包使用国内源更快：
    * 清华源（首选）：pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
    * 阿里源pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
* zh_core_web_sm，zh_core_web_md 和 zh_core_web_lg 是 spaCy 提供的不同大小的中文预训练模型。这些模型的区别在于模型的大小和性能。模型越大，通常性能越好，但同时会占用更多的内存和计算资源。以下是这三个模型的简要概述：    
    1. zh_core_web_sm：这是一个较小的模型，适用于快速原型开发和在资源有限的环境中使用。它的性能可能不如较大的模型，但在许多任务中表现仍然可以接受。
    2. zh_core_web_md：这是一个中等大小的模型，提供了在性能和资源占用之间的平衡。它的性能通常优于较小的模型，但占用的资源也相应更多。
    3. zh_core_web_lg：这是一个较大的模型，通常具有最佳性能，但占用的内存和计算资源也最多。它适用于对性能要求较高的应用程序，例如在生产环境中使用。
* zh_core_web_trf 是 spaCy 提供的一个基于 Transformer 的中文预训练模型。与 zh_core_web_sm，zh_core_web_md 和 zh_core_web_lg 不同，这个模型是基于更先进的 Transformer 架构（如 BERT、RoBERTa 等）进行训练的。Transformer 模型在许多自然语言处理任务中表现出优越的性能，但对硬件要求更高。
