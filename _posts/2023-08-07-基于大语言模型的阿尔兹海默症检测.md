---
layout: post
title:   基于大语言模型的阿尔兹海默症检测
date:   2023-08-07 22:10:20
categories: Emotion
toc: true
---

基于该主题的有三个报告

# 报告1：Predicting Dementia from Speech using LLMs，梁化楼教授，德雷塞尔大学生物医学工程

## 基本信息

* Agbavor, F., & Liang, H. (2022). Predicting dementia from spontaneous speech using large language models. PLOS Digital Health, 1(12), e0000168. https://doi.org/10.1371/journal.pdig.0000168
* 基于英语的预测研究：Liberman 2014
* 基于普通话的研究：Yuan 2016 2020
* 梁化楼老师实验室网站：https://liang-lab.org/

## 报告笔记

### 前言

* Agbavor & Liang进行这项研究的核心目标是:
    * 利用大规模预训练语言模型(GPT-3等)进行阿尔茨海默病检测,以突破基于手工特征和领域知识的方法的局限性（domain specific knowledge and hand-crafted transformations）。
    * 前人的特征工程方法依赖专业领域知识且难以泛化。而预训练语言模型可以无监督从大规模文本中学习语言表示,并可以迁移到下游任务（downstream tasks）中。
    * 该研究的最大创新就是首次探索了GPT-3在病人语音分析中的潜力,证明其可以跨领域地学习相关的语言表示,实现对阿尔茨海默病患者的有效检测。
* 论证GPT-3相比传统方法的优越性，作者主要说了两点：
    第一，更好的泛化能力。GPT-3通过无监督在大规模文本中进行预训练,可以进行零资料(zero-shot)学习,即不需要任务特定的数据就可以迁移到下游任务。这展示了其强大的跨领域泛化能力。
    第二，编码丰富的语义知识。GPT-3可以学习输入文本的语义表示,编码大量世界知识。这可以帮助模型学习到显式特征无法表达的语义信息,增强下游任务的判别能力。
相比之下,前人在阿尔茨海默病检测中使用的特征工程方法,依赖于人工设计的领域特征,泛化能力和语义编码能力都较弱。因此,GPT-3无监督的预训练机制和语义建模能力,是其相比特征方法最大的优势,可以应用到病人语音分析中,达到更好的检测性能。
* 显示特征与GPT-3 word embeddings的区别
    * 显式特征在英文中并没有一个统一的固定说法。显式特征相对于隐式特征,指的是那些可以直接观测和量化的特征。比较恰当的英文表达是：
        * Observable features
        * Quantifiable features
        * Hand-crafted features
        * Engineered features
    * 显示特征主要指前人研究中使用的手工设计的语音和语言特征,例如:
        * 语音的声学特征(音高、音强、语速等)
        * 文本的词频、词性、句法特征等
        * 一些不可解释的黑盒模型提取的特征。
    * 这些特征都属于显式的、可解释的特征,可以明确定义和解释其语义。
而GPT-3学习的语义表示则是隐式的、不可解释的,它综合编码了文本的深层语义关系,这些知识是人工特征难以表达的。
所以前面提到的"显式特征",主要指的是可解释性较强,可人工构建的特征,这些特征相比GPT-3等模型学习的隐式语义表示,表达能力和泛化能力较弱。

### Methods

* 基本流程：输入（prompt） to  GenerativeAI to Outputs
* 数据采集：经典的看图（cookie theft picture）说话任务（Goodglass 2001；Becker 1994）。
    * Goodglass H, Kaplan E, Weintraub S. BDAE: The Boston Diagnostic Aphasia Examination. Third. Lippincott Williams & Wilkins Philadelphia, PA; 2001.
    * Becker JT, Boiler F, Lopez OL, Saxton J, McGonigle KL. The Natural History of Alzheimer’s Disease:
Description of Study Cohort and Accuracy of Diagnosis. Arch Neurol. 1994 Jun 1; 51(6):585–94.
* 样本量：
    * 总样本量：There are totally 237 speech recordings, with 70/30 split balanced for demographics, resulting in 166 and 71 in the training set and the test set, respectively. 即一共237个被试的录音，3-7分为包含71个和166个被试数据的测试集和训练集。
    * 训练集：In the training set, there are 87 samples from AD subjects and 79 from nonAD (or healthy control) subjects.
* 从被试录音到word embeddings（speech to text）的主要步骤可能如下:
    * 语音数据采集:对阿尔茨海默病患者和正常人群进行语音录制,获得原始语音数据。
    * 语音识别:使用自动语音识别模型(Wav2Vec 2.0),将语音转录为文本。具体过程又分为以下几个过程：
          * 使用预训练语音识别模型Wav2Vec 2.0。该模型是当前语音识别领域的 state-of-the-art 模型。state-of-the-art 模型指在该领域当前技术水平最高,性能最优的模型。例如论文提到的Wav2Vec 2.0 是语音识别领域当前性能最好的模型,代表了该领域的最新技术进展,所以将其描述为"state-of-the-art"。
          * 选用该模型的wav2vec2-base-960h版本,该版本是在960小时LibriSpeech语音数据集上进行预训练和微调的。
          * 使用librosa库加载语音波形数据。librosa是专门用于分析声音的python包。
          * 使用Wav2Vec2Tokenizer对载入的波形进行tokenize。
          * 如果必要,将波形切分成较小的chunk(本研究中最大大小为100,000),以适应内存。
          * 将切分好的波形输入到Wav2Vec2ForCTC模型中进行语音识别。
          * Wav2Vec2ForCTC模型将语音解码输出为文本转录。
          * 最终获得语音对应的文本转录数据。
    * 嵌入映射:使用预训练的大语言模型(chatGPT3等),为每个词汇生成词向量。
          * 将speech-to-text转换得到的文本传给GPT-3模型。
          * GPT-3模型会为文本中的每个词汇生成一个词向量(word embedding)。
    * 构建词嵌入矩阵:将所有词汇的词向量连接,构成词嵌入矩阵(word embeddings matrix)，用来表示整个训练集。
    注意，通常情况下,一个预训练语言模型会为整个训练集构建一个统一的词向量矩阵,而不是为每个被试各自构建词向量矩阵。
    这个统一的词向量矩阵可用于任意被试样本的表征。
    * 构建数据集:将词嵌入矩阵与对应的标签组合,构建 294 语音识别与语音分析
    * 训练模型:基于词嵌入矩阵训练分类模型。

* word embeddings的数量
    * 训练集:91,000条转录(阿尔茨海默病患者10,000条,健康对照81,000条)
    * 测试集:10,000条转录(阿尔茨海默病患者1,000条,健康对照9,000条)
* 数据分析：
    * 基本思路：使用LLMs进行分类和回归
    * 基本流程：text embeddings → 对chatGPT进行一下微调（fine tuning）→ 文本embeddings之间的associations

### 结果

* AD在描述故事时会颠三倒四，多重复性内容，不确定，不大敢说
* 不流利，具体表现为时间短、中止多（例如啊此类的语气词）、停顿多
* 单独的GPT3-词嵌入（GPT-3 embeddings）特征效果好于单独的声学特征(acoustic features)。
* 语义+语音特征的整合，对于预测率没有显著改善。但加入声学特征后,如果以recall降低为代价,可以提高预测率（precision）。
    * recall称为Sensitivity或True Positive Rate, 反映的是真正例中被正确识别出的比例,分母是真正例的总数。定义为: Recall = 真正例预测正确数 / 真正例总数 = True Positive / (True Positive真正例 + 漏诊False Negative)
    * precision衡量的是,在模型预测出的正例(例如病人)样本中,实际真正正确的比例。Precision = 真正例预测正确数 / 预测为正例总数 = True Positive / (真正例True Positive + 假正例False Positive)
    * 通常precision和recall存在trade-off,precision提高时recall可能下降,反之亦然。建模时通常需要在precision和recall之间找到平衡。

# 报告2：利用停顿和包含停顿编码的大语言模型识别阿尔兹海默病，袁家宏，中国科技大学人文与社会科学学院教授

## 基本信息

* 袁家宏老师实验室网站：http://hsss.ustc.edu.cn/2022/1122/c20453a581925/page.htm
* 论文基本信息：Yuan, J., Cai, X., & Church, K. (2021, June). Pause-encoded language models for recognition of alzheimer’s disease and emotion. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 7293-7297). IEEE.
* 与梁教授的区别：
    * 人工转写，以保证对语音停顿的编码
    * 梁教授：机器转写

## 论文信息

1. 研究的科学问题。这篇论文的作者进行这个研究的主要原因和创新点如下:
    * 原因:利用停顿信息改进阿尔茨海默病检测和情感识别的性能。
    * 创新点:提出了停顿编码语言模型,通过在预训练语言模型中插入停顿符号,让模型学习停顿信息。是第一个利用停顿信息来同时改进病人语音的病症检测和情感识别的研究。
    * 要解决的核心科学问题:
        1. 停顿信息如何帮助检测病人的语言障碍和情感状态?
        2. 如何有效地将停顿信息融合到语言模型中?
        3. 停顿编码预训练语言模型与标准模型相比,可以带来多大的性能提升?
相比于前人研究,该论文最大的创新在于提出了停顿编码（Pause-encoded language models）的想法,并通过预训练语言模型有效学习停顿信息,从而改进下游任务。该研究解决了如何利用停顿信息的科学问题,为病人语音分析提供了新思路。

2. 样本量:
    总样本量:542个语音样本；每个语音样本（录音）对应一个被试的一次发言。
    训练集:392个(阿尔茨海默病患者192,健康对照200)
    测试集:150个(阿尔茨海默病患者50,健康对照100)

3. 与梁教授的论文差异。Yuan 2021和Agbavor 2022这两篇研究主要的区别有:
    * 语音特征的差异
        Yuan 2021使用了停顿编码的语言模型特征。
        Agbavor 2022使用大规模语言模型提取的语义特征。
    * 模型设计的不同
        Yuan 2021采用停顿编码的预训练语言模型。
        Agbavor 2022基于GPT-3等大模型微调得到分类模型。
    * 目标任务的差异
        Yuan 2021同时检测阿尔茨海默病和情感识别。
        Agbavor 2022仅针对阿尔茨海默病识别。
    * 数据规模的不同
        * Yuan 2021使用较小规模的数据集。
        * Agbavor 2022使用大规模医疗语音转录数据集。
        * 正是由于数据集规模的原因，Yuan等人可以对小规模数据进行人工转写；而Agbavor等人只能对大规模数据进行机器转写。
    * 性能差异
        * Agbavor 2022的大模型精度更高。
        * 但Yuan 2021提供了多任务的结果。
        * 由此可知，影响模型最后精度的，不仅是数据量，还有数据的精确程度。
总体来说,两篇研究都使用语言模型技术,但Agbavor 2022使用了更大规模的预训练模型和数据集,取得了更高的阿尔茨海默病检测性能,是更加强力的工作。而Yuan 2021探索了停顿编码的创新点,并实现了多任务学习。

# 报告3：基于语音准语言特征的阿尔兹海默病检测，张卫强，清华大学副研究员

## 基本信息

* 个人主页：http://web.ee.tsinghua.edu.cn/wqzhang
* 语音与音频技术实验室（SATLab，Speech and Audio Technology Lab）主页：http://web.ee.tsinghua.edu.cn/satlab/en/index.htm
* 报告题目：基于语音的阿尔兹海默病检测
* 报告论文：Chen, X., Pu, Y., Li, J., & Zhang, W. Q. (2023, June). Cross-Lingual Alzheimer’s Disease Detection Based on Paralinguistic and Pre-Trained Features. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-2). IEEE.
* 论文介绍：http://web.ee.tsinghua.edu.cn/satlab/zh_CN/article/6966/content/1461.htm#article

## 论文思路

1. AD的严重症状与发展趋势
2. AD的常用检测方法
    * 认知功能评估
    * 神经影响分析
    * 生物标志物分析
    * 遗传物质分析
3. 常用评估方法共同的局限性
    * 流程复杂
    * 成本高昂
    * 医疗资源分配不均，不便于普惠大众
4. 所以AD的早期筛查非常重要。早发现，早干预，延缓AD的发病进程。
5. 语音检测技术已经在情感检测和抑郁检测中应用，证明其可行性，且具有以下优势：
    * 非侵入
    * 低成本
6. 主要研究目的：
    * 参加ICASSP-SPGC-2023 ADReSS-M挑战赛任务。该挑战赛涉及在训练集测试集不匹配的设置下对AD患者和健康对照进行分类以及预测MMSE得分。
    * 探究哪些声学特征可以在不同语言之间进行泛化和转移,用于阿尔茨海默病(AD)预测。
    * 评估多语言预训练模型在跨语言AD检测中的效果。作者检验了XLSR-53和RoBERTa等模型,以查看预训练特征是否可以跨语言转移。
    * 探索多语言AD检测中的准语言特征（paralinguistic features）。作者假设准语言特征可以反映AD患者的变化,并可以跨语言推广。准语言特征在这个研究中指的是语音中的一些非语言信息特征,包括语调、语速、音量、语气等方面的特征。准语言特征不同于语音的语言内容本身,它更多反映说话者的情绪状态、个人风格等信息。研究人员的假设是,阿尔茨海默病患者在这些准语言特征表现上会有异常,例如语速变慢,语调单调等。而这些特征的变化可能与语言无关,在不同语言的患者中表现类似。
7. 语料特点：这个ADReSS挑战赛的训练集是英语,包含了英语母语患有阿尔茨海默病和健康人士的语音样本。但是测试集是希腊语,同样包含了希腊语母语患病者和健康人的语音。
所以研究人员面临的挑战是,他们只有英语训练集可以用来建模和训练,但是需要利用这些模型去预测希腊语测试集中的语音属于阿尔茨海默病患者还是健康人士。这种训练集测试集语言不匹配的设置增加了研究难度。研究人员期望能够找到一些泛化性强的语音特征,在英语训练集中进行学习,然后可以直接迁移到希腊语语音中,实现跨语言的病人检测。这就是为什么他们会探索准语言特征和预训练语音模型特征,希望找到一些跨语言可迁移的表示。
8. 根据这篇论文,作者解决跨语言阿尔茨海默病检测这个研究问题的思路和具体实现流程大致如下:
    * 思路:使用准语言特征和预训练语音模型特征,这些特征可能跨语言保持一致,因此可以训练英语模型后应用到希腊语测试集。
    * 数据:论文使用了ADReSS挑战赛提供的英语训练集和希腊语测试集。
    * 特征提取:使用openSMILE工具从语音中提取了三类准语言特征;使用预训练语音模型XLSR-53从语音中提取预训练特征。
    * 模型训练:将上述特征作为输入,分别训练了支持向量机(SVM)分类模型和全连接神经网络回归模型。
    * 预测:将希腊语测试集语音输入模型,提取相应特征,然后输入训练好的模型进行阿尔茨海默病分类或MMSE评分预测。
    * 提交结果:将模型预测结果提交到ADReSS评测系统,得到分类准确率和回归RMSE作为最终评价指标。
    * 分析:分析不同特征在跨语言任务中的效果,准语言特征效果较好,说明其跨语言迁移能力强。
    * 结论:通过准语言特征可以实现英语模型到希腊语的迁移,实现跨语言阿尔茨海默病检测。
    以上是论文中使用准语言特征和预训练特征进行病人检测的思路和流程。研究证明了准语言特征在这个任务中的效果较好,有望实现多语言痴呆检测。
9. 预训练特征（Pre-Trained Features）和准语言特征（Paralinguistic Features）是这个论文中使用的两类不同类型的语音特征,主要区别如下:
    * 定义不同:
      * 预训练特征:通过预训练语音模型(如XLSR-53)从语音中提取的语义特征表示,反映语音包含的语言信息内容。
      * 准语言特征:主要反映说话者情绪、语调语气等非语言信息的语音特征。
    * 获得方式不同:
      * 预训练特征:需要通过训练好的语音模型来映射语音到特征空间。
      * 准语言特征:可以通过信号处理方法直接从语音中提取。
    * 语言依赖性不同:
      * 预训练特征:与特定语言耦合较强,跨语言迁移能力较弱。
      * 准语言特征:反映说话者状态,可跨语言保持一致。
    * 作用机制不同:
      * 预训练特征: encodes语音语义内容,反映语言障碍。
      * 准语言特征:反映说话方式,可检测非语言症状。
    * 效果不同:在论文中,准语言特征效果较好,更适合跨语言任务。
综上,两者属于不同类型语音特征,准语言特征更具跨语言迁移力,适合该研究的目标。


# 总结

## Acoustic features and Paralinguistic features

* 疑问：Agbavor 2022论文中的Acoustic feature与Chen et al (2023). Cross-Lingual Alzheimer’s Disease Detection Based on Paralinguistic and Pre-Trained Features. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-2). IEEE.文章中的Paralinguistic features，有什么区别？综合来看，Agbavor 2022论文发现语义特征能够更加准确地预测AD分类，而chen等人（2023）却发现准语言特征比语义特征能够更加准确地进行跨语言预测AD，两个研究的结论是否冲突呢？
* Agbavor 2022的Acoustic feature和Chen 2023的Paralinguistic features作为语音的声学特征,在本质上是相似的,都反映了语音在音高、音强、语速等方面的特征。
* 两者的区别主要在于:
    * Chen 2023工作更专注于语音的情感和语调方面的特征,而Agbavor 2022考察更全面的声学特征。
    * Chen 2023的特征面向跨语言任务,而Agbavor 2022主要用于单语言。
* 至于两个研究在语义特征和声学特征上的结论不同,我认为不一定存在冲突:
    * 两项研究的任务不同 - Agbavor 2022面向普通病人识别,Chen 2023面向跨语言识别,后者增加了语言转换的难度。
    * 声学特征更语言无关,在跨语言任务中效果更好是合理的。而语义特征依赖语言,跨语言效果较差也在预期中。
    * 两项研究都证实结合语义和声学特征效果最好,说明两者提供了互补的信息。
* 综上所述,这两项研究结论的不同之处主要是由于任务的不同带来的,两者并不真正存在对立。结合使用语义和声学特征仍是更好的选择。
