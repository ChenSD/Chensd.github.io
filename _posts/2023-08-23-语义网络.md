---
layout: post
title:   语义网络分析
date:   2023-08-07 22:10:20
categories: language
toc: true
---

## Basic information

* 语义网络（semantic network）是一种知识结构，它描述了概念之间的相互关系，并说明了概念之间的相互联系
* 语义网络是一个有向或无向图，由代表概念的顶点（nodes）和代表概念间语义关系的边（edges）组成，映射或连接语义场（semantic field）。

## Paper1

* Miani, A., Hills, T., & Bangerter, A. (2022). Interconnectedness and (in) coherence as a signature of conspiracy worldviews. Science Advances, 8(43), eabq3668.

### 研究思路

* 作者的核心思路在于，如何把阴谋论文本网络化，网络的节点和连接应该对应什么概念，如何量化和计算？
* 理解该文章思路的关键在于搞清楚，作者如何利用种子点和主题词的共现（the co-occurrences of seeds and LDA topics）来进行语义网络的构建。
* 语义网络
    * 节点（nodes）： seeds vs. topics
    * 边（edges）：the co-occurrences of seeds and LDA topics。
    * 数据格式：行（rows）是文件/报告（documents） * 列是seeds or topics。例如，有100篇报道，每篇报道有10-20个主题。
    * 关键的操作，如何从topics相关矩阵，转换得到co-occurrences矩阵。
* topics和seeds的向量化。此处应当了解，类似于word embeddings的概念，每个topic或seed在数据中都应该进行了向量化。然后，不同的topics之间就可以进行perason相关计算。
例如吴孟达和周星驰之间的相关，就应该高于梦露和周星驰之间的相关。
* 背景知识：在LDA主题模型中，每个主题都是一个概率分布，表示该主题下单词出现的概率。这个概率分布可以看作是一个向量，其中每个元素表示该单词在该主题下的概率。例如，假设有一个主题关于体育，它的关键词分布可能是：“篮球：0.2，足球：0.1，棒球：0.05，跑步：0.03，运动：0.02”等。
同样地，种子词也可以表示成向量形式。例如，假设有3个种子词：“健康”、“锻炼”和“饮食”，它们可以表示为一个3维向量：“健康：[0.5, 0.3, 0.2], 锻炼:[0.2, 0.3, 0.5], 饮食:[0.1, 0.2, 0.7]”。这种向量的表示方式有助于在主题模型中进行数学运算和统计推理。通过向量运算，可以计算单词之间的相似性、主题之间的相关性等。
* 为了解释如何从between-topic correlation matrix转换到co-occurrence matrices，我们可以考虑以下步骤。
    1. 首先，假设我们已经得到了一个between-topic correlation matrix，其中每个元素表示一对主题之间的相关性。
    2. 然后，我们可以将这个matrix扩展到包含所有主题对的相关性。在这个扩展matrix中，每行表示一个主题与其他所有主题的相关性。
    3. 接下来，我们可以将这个扩展matrix转换为co-occurrence matrices。具体来说，我们可以为每个文档创建一个co-occurrence matrix，其中每个元素表示一对关键词在该文档中的共现频率。
    4. 最后，我们可以使用这些co-occurrence matrices来构建共现网络，其中每个节点表示一个关键词，每条边表示一对关键词之间的共现关系。
以下是一个简单的例子来说明这个过程：

假设我们有一个包含三个主题A、B和C的between-topic correlation matrix：

```css
| A | B | C |
|---|---|---|
| 0.8 | 0.2 | 0.1 |
| 0.2 | 0.8 | 0.3 |
| 0.1 | 0.3 | 0.8 |
```
然后，我们将这个matrix扩展到包含所有主题对的相关性：

```css
| A | B | C | AB | AC | BC | ABC |
|---|---|---|----|----|----|-----|
| A | 0.8 | 0.2 | 0.8 | 0.2 | 0.2 | 0.8 |
| B | 0.2 | 0.8 | 0.2 | 0.8 | 0.3 | 0.3 |
| C | 0.1 | 0.3 | 0.1 | 0.3 | 0.3 | 0.1 |
```
接下来，我们为每个文档创建一个co-occurrence matrix：

假设我们有一个文档d，其中包含了主题A、B和C的关键词，并且这些关键词在文档中以以下顺序出现：A -> B -> C -> A -> B -> C -> A。

那么，对应的co-occurrence matrix如下：

```css
| A | B | C | AB | AC | BC | ABC |
|---|---|---|----|----|----|-----|
| 1 | 1 | 1 | 1 | 1 | 1 | 1 | A -> B -> C -> A -> B -> C -> A
```
最后，我们可以使用这些co-occurrence matrices来构建共现网络。在这个例子中，我们只有一个co-occurrence matrix，所以我们可以直接使用该matrix构建一个包含三个节点的共现网络。

### 研究方法探讨

* 作者使用R包`topicmodels`来进行主题提取。以下是一个简单的例子，说明如何使用该包进行主题提取：

```R
# 安装和加载topicmodels包
install.packages("topicmodels")
library(topicmodels)

# 加载LOCO语料库
data("loco")

# 对语料库进行预处理
preproc <- function(words) {
  words <- tolower(words)
  words <- tm_map(words, content_transformer(removePunctuation))
  words <- tm_map(words, content_transformer(removeNumbers))
  words <- tm_map(words, content_transformer(removeWords), stopwords("smart"))
  words <- tm_map(words, content_transformer(stripWhitespace))
  return(words)
}
loco <- preproc(loco)

# 创建语料库矩阵
matrix <- TermDocumentMatrix(loco)

# 使用LDA算法进行主题提取
lda_model <- LDA(matrix, k = 5) # 这里选择5个主题

# 输出主题信息
print(lda_model)

# 输出每个主题中概率较高的单词
topics <- topics(lda_model)
```
在上述代码中，首先加载了`topicmodels`包，然后加载了LOCO语料库。接下来，定义了一个预处理函数`preproc`，用于对语料库中的文本进行预处理，包括转换为小写、去除标点符号、去除数字、去除停用词和去除空白。然后，对LOCO语料库进行预处理。
接下来，创建了一个语料库矩阵，然后使用`LDA()`函数进行主题提取。在`LDA()`函数中，指定了矩阵和主题数量（k值）。然后，通过打印模型信息可以查看主题信息。
最后，使用`topics()`函数提取每个主题中概率较高的单词，这将给出每个主题的主题分布。

* 在该文中，Seeds和LDA topics有和区别和联系，请举例说明
在文中，Seeds和LDA topics是两个不同的概念。Seeds是指作者在LOCO语料库中选取的一组关键词，用于初始化LDA模型的主题分布。而LDA topics则是通过运行LDA模型后从语料库中提取出来的主题。
Seeds是一组具有代表性的单词，它们在LOCO语料库中出现频率较高，可以代表该语料库中的重要信息。作者选取了35个Seeds，并将其分为五组，每组七个单词。Seeds的作用是提供主题分布的初始值，以加速LDA模型的收敛。
LDA是一种基于概率的主题模型，它通过对文本中的词频和词序进行分析，推断出文本的主题。在运行LDA模型后，从LOCO语料库中提取出了五个主题。这些主题涵盖了不同的领域和话题，例如第一主题涉及医疗、保健和疾病，第二主题涉及社会、文化和媒体，第三主题涉及政治、法律和权力，第四主题涉及经济、商业和市场，第五主题涉及环境、生态和自然。
虽然Seeds只是用于初始化LDA模型的主题分布，但Seeds的选择对于主题的提取具有一定的影响。例如，如果Seeds的选取不具有代表性或过于局限于某一领域，可能会导致主题分布的不均衡或偏差。因此，Seeds的选择应该考虑到语料库的多样性和代表性。
总之，Seeds和LDA topics是不同的概念，但Seeds的选择对于LDA模型的运行和主题提取具有一定的指导作用。
* 根你的回答我，我感觉作者应该先确定seeds，在根据seeds确定topics。但是实际上，作者先进行了topic extraction，再进行了seeds extraction。在topic extraction的过程中，似乎并没有用到seeds？
你是对的，作者在文章中的步骤是先进行了主题提取(topic extraction)，然后再进行了种子词提取(seeds extraction)。在这个过程中，确实没有直接使用Seeds来提取Topics。
实际上，Seeds主要用于初始化LDA模型的主题分布，这对模型的结果会有一定影响。但并不是说必须要先确定Seeds，再提取Topics。你也可以直接对整个语料库运行LDA模型，而不需要事先确定Seeds。
作者可能选择先进行主题提取，然后再提取种子词，是为了确保提取的Topics能尽可能地反映语料库的真实主题，而不是被Seeds所引导。因为Seeds虽然可以提供初始的主题分布，但如果Seeds的选择不够准确，可能会影响到最终的主题提取结果。
总的来说，虽然Seeds和Topics有联系，但它们是两个不同的步骤，而且它们的提取顺序并不是固定的，可以根据实际需求进行调整。
* 我感觉作者是将seeds与topics视为两种并行的关系，前者是基于先验概率的关键词，后者是基于后验概率的关键词。作者后面同时进行了基于seeds和基于topics的网络构建，这可能是为了避免方法偏倚，尽可能保证所构建的语义网络反映了实际的文本情况。
* 通过LDA主题提取，构建共现网络——co-occurrences of LDA topics是什么意思，请举例说明
"Co-occurrences of LDA topics"指的是LDA主题的共现。LDA（Latent Dirichlet Allocation）是一种主题模型，用于从大量文本中提取主题。在LDA中，每个主题都由一组概率分布的关键词构成。当一个文档涵盖了多个主题时，这些主题的关键词可能会在该文档中同时出现，这就形成了主题的共现。
举例来说，假设我们有一个关于电影的语料库，我们使用LDA从中提取了三个主题，分别是"科幻"、"动作"和"喜剧"。在某个文档中，如果同时出现了"星球大战"、"詹姆斯·邦德"和"周星驰"这三个关键词，那么我们就可以说这三个主题（"科幻"、"动作"和"喜剧"）在该文档中发生了共现。
Co-occurrences of LDA topics可以帮助我们理解不同主题之间的联系和相互影响。通过分析主题共现的情况，我们可以发现某些主题之间可能存在的关联或者排斥关系，这对于理解文本的整体结构和语义内涵具有重要的意义。

* 作者具体如何使用igraph构建共现网络，举例说明
作者具体使用igraph构建共现网络的方法如下：

1. 安装和加载igraph包：首先，安装igraph包，然后加载igraph包。


```R
install.packages("igraph")
library(igraph)
```
2. 构建共现关系矩阵：使用上一步中的共现关系矩阵构建igraph对象。


```R
cooccurrence_matrix <- matrix(cooccurrences, nrow = length(seeds), ncol = length(seeds))
ig_graph <- graph_from_adjacency_matrix(cooccurrence_matrix, mode = "undirected", weighted = TRUE)
```
这里，`cooccurrences`是关键词的共现频率矩阵，`seeds`是种子词列表。`graph_from_adjacency_matrix()`函数根据共现矩阵创建一个无向图，其中每个节点对应一个种子词，每个边对应两个种子词之间的共现关系。`mode = "undirected"`表示创建无向图，`weighted = TRUE`表示边的权重是基于共现频率的。

3. 可视化共现网络：使用igraph的函数对共现网络进行可视化。


```R
plot(ig_graph, vertex.size=10, vertex.label.cex=0.8)
```
这里，`plot()`函数用于绘制图形，`vertex.size=10`表示节点的大小为10，`vertex.label.cex=0.8`表示节点标签的字体大小为0.8倍。

举例来说，假设我们有一个关于电影的语料库，我们使用LDA从中提取了三个主题，分别是"科幻"、"动作"和"喜剧"。假设种子词列表为`seeds = c("星球大战", "詹姆斯·邦德", "周星驰")`，关键词的共现频率矩阵为：


```csharp
cooccurrences = array(c(30, 20, 10, 15, 25, 10, 5, 5, 5, 5), dim = c(3, 4))
```
那么，我们可以按照上述步骤构建一个igraph对象，并使用`plot()`函数对其进行可视化。结果将显示一个包含三个节点的无向图，节点对应于种子词"星球大战"、"詹姆斯·邦德"和"周星驰"，边对应于这些种子词之间的共现关系。边的权重将根据共现频率进行赋值。

## Paper2 大脑中语义网络的构建

* 最近，Huth等人(2012;2016)开发了一种方法，通过使用自然语言处理技术对功能磁共振成像（fMRI）大脑活动进行建模，可以定量评估大脑中单个单词的语义表示（即，大脑单词嵌入，word embeddings in the brain）。该方法启发我们通过评估大脑词嵌入之间的相似性来表征人脑中的语义网络(Schematic Network)，并使用网络分析分析其结构特征。
* Matsuto等人（2023）使用上述方法，发现精神分裂症个体的语义网络相比正常个体，不具有小世界属性，呈现出模块化的特征，即结构更加混乱。
* Huth AG, Nishimoto S, Vu AT, Gallant JL. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron. 2012;76(6):1210–1224.
*	Huth AG, De Heer WA, Griffiths TL, Theunissen FE, Gallant JL. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature. 2016;532(7600):453–458.
* Matsumoto, Y., Nishida, S., Hayashi, R., Son, S., Murakami, A., Yoshikawa, N., Ito, H., Oishi, N., Masuda, N., Murai, T., Friston, K., Nishimoto, S., & Takahashi, H. (2023). Disorganization of Semantic Brain Networks in Schizophrenia Revealed by fMRI.Schizophrenia bulletin, 49(2), 498–506. https://doi.org/10.1093/schbul/sbac157
* 国内报道：https://mp.weixin.qq.com/s/etKNlDcQvxhWCttglNrVcA

### 研究思路

* 整体的思路如下：
    1. 将语言任务中的高频词，通过word2vec向量化，得到感兴趣语料的向量矩阵；
    2. fMRI多体素模式分析：将电影场景引起的神经活动的多体素模式建模为场景向量矩阵和模型权重的矩阵乘积
    3. 利用体素编码模型的权重对每位受试者的大脑词嵌入进行量化。首先，选取预测准确率最高的前 7000 个体素来创建权重矩阵。然后，将电影场景描述中出现频率最高的 11132 个单词中的前 1000 个单词在 word2vec 向量空间中的原始单词向量乘以模型权重，得到每个单词在大脑中的向量表示。
