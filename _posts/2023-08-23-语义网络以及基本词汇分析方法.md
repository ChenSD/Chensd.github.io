---
layout: post
title:   语义网络以及基本词汇分析方法
date:   2023-08-07 22:10:20
categories: language
toc: true
---

# 1. 语义网络的基本理论和概念

## 语义角色

* “语义角色”（semantic role/semantische Rolle，或称主题关系，thematic relations）的概念是美国语言学家Charles J. Fillmore于1968年提出的。
https://zhuanlan.zhihu.com/p/42401502


## 语义的网络

* 语义网络分析（Semantic Network Analysis）是社会网络分析技术在文本分析中的一种运用。语义网络分析背后的假设是在文本中频繁共现的词语或概念之间暗含着某种联系，而这些联系可以通过一些统计指标（如共现频率）来进行测量。这一方法有助于社会科学研究者通过词语之间的联系，识别出文本内包含的话题(Topic)或框架(Frame)。
* 语义网络（semantic network）是一种知识结构，它描述了概念之间的相互关系，并说明了概念之间的相互联系
* 语义网络是一个有向或无向图，由代表概念的顶点（nodes）和代表概念间语义关系的边（edges）组成，映射或连接语义场（semantic field）。语义场（semantic field）是现代语义学中非常重要的理论，由德国语言学家特雷尔（J.Trier）在20世纪三十年代提出。它指的是归属于一个共同概念之下的、意思上紧密相联并相互制约的一组词的聚合体，也称为义场、词汇场或词场。在一个语义场中，每个词都有共同的概念特征，这些特征使得它们在意义上相互关联并相互制约。语义场可以通过不同的词之间的对比，根据它们词义的共同特点或关系划分出来。例如，“胳膊”、“胸”、“腹”、“脚”、“头”、“腿”等词都属于“人体器官”这个语义场。另一个例子是关于“人的长相”的语义场，包括“美丽”、“丑陋”、“秀气”、“清秀”、“俊”、“英俊”等词。这些词都在描述人的外貌特征，因此它们属于同一个语义场。总之，语义场是一种将词汇按照其意义进行分类和组织的方式，它有助于我们理解和分析语言中的词汇和语义关系。
* 语义网络分析的基础是识别文本中的词语联系。以中文文本为例，由于中文词语之间不存在自然间隔，所以在进行分析前，我们需要将句子切分成单个词语的组合，并将词语作为网络中的节点（node）。
* 其中，某些不具有实质性含义的停用词（stopword）需要清理（比如“而且”），而某些具有同样含义的词语则需要统一（如“真的”和“真实”）。然后，我们需要找出词语之间的关联，比如两个字在句子中是相邻的就是联系的一种方式。词语之间的关联可以作为网络中的边（edge）。

* https://mp.weixin.qq.com/s/WITFYQY5IWyV67RIlJw0nQ

## 基本文献

* Borsboom, D., Deserno, M. K., Rhemtulla, M., Epskamp, S., Fried, E. I., McNally, R. J., Robinaugh, D. J., Perugini, M., Dalege, J., Costantini, G., Isvoranu, A.-M., Wysocki, A. C., van Borkulo, C. D., van Bork, R., & Waldorp, L. J. (2021). Network analysis of multivariate data in psychological science. Nature Reviews Methods Primers, 1(1), Article 1. https://doi.org/10.1038/s43586-021-00055-w
* Borsboom, D., Rhemtulla, M., Cramer, A. O., van der Maas, H. L., Scheffer, M., & Dolan, C. V. (2016). Kinds versus continua: A review of psychometric approaches to uncover the structure of psychiatric constructs. Psychological medicine, 46(8), 1567-1579.
* Boschloo, L., van Borkulo, C. D., Borsboom, D., & Schoevers, R. A. (2016). A prospective study on how symptoms in a network predict the onset of depression. Psychotherapy and psychosomatics, 85(3), 183-184.
* Borsboom, D., Robinaugh, D. J., Group, T. P., Rhemtulla, M., & Cramer, A. O. (2018). Robustness and replicability of psychopathology networks. World Psychiatry, 17(2), 143.
* Van Borkulo, C. D., van Bork, R., Boschloo, L., Kossakowski, J. J., Tio, P., Schoevers, R. A., Borsboom, D., & Waldorp, L. J. (2022). Comparing network structures on three aspects: A permutation test. Psychological Methods. DOI: 10.1037/met0000476
* Johal, S. K., & Rhemtulla, M. (2021). Comparing estimation methods for psychometric networks with ordinal data. Psychological Methods, No Pagination Specified-No Pagination Specified. https://doi.org/10.1037/met0000449 （序列数据的网络分析-例如通过李克特量表采集的问卷数据都是序列数据）

* van Borkulo, C. D., van Bork, R., Boschloo, L., Kossakowski, J. J., Tio, P., Schoevers, R. A., Borsboom, D., & Waldorp, L. J. (2022). Comparing network structures on three aspects: A permutation test. Psychological Methods, No Pagination Specified-No Pagination Specified. https://doi.org/10.1037/met0000476

* Quinn, K., & M. Powers, R. (2016). Revisiting the concept of ‘sharing’for digital spaces: An analysis of reader comments to online news. Information, Communication & Society, 19(4), 442-460. 2016年，Quinn和Powers在一篇关于网络分享概念的研究中使用了该方法。为了探究数字化时代下人们如何理解“分享”这一概念，两位研究者下载了纽约时报网站中关于“分享”的文章与这些文章的在线评论文本，并在清理文本后对数据展开了语义网络分析（见图1）。图中每一个点对应了一个词语，词语之间的连线就代表这两个词曾经一起出现。

* Chen, Z., Su, C. C., & Chen, A. (2019). Top-down or bottom-up? A network agenda-setting study of Chinese nationalism on social media. Journal of Broadcasting & Electronic Media, 63(3), 512-533.
* 郑雯, 桂勇, & 黄荣贵. (2019). 论争与演进: 作为一种网络社会思潮的改革开放——以 2013-2018 年 2.75 亿条微博为分析样本. 新闻记者, 1.

## 语义网络在创造力中的应用

* 经典研究范式： Benedek, M., Kenett, Y. N., Umdasch, K., Anaki, D., Faust, M., & Neubauer, A. C. (2017). How semantic memory structure and intelligence contribute to creative thought: A network science approach. Thinking & Reasoning, 23(2), 158-183.
* Kenett, Y. N., Levy, O., Kenett, D. Y., Stanley, H. E., Faust, M., & Havlin, S. (2018). Flexibility of thought in high creative individuals represented by percolation analysis. Proceedings of the National Academy of Sciences, 115(5), 867–872. https://doi.org/10.1073/pnas.1717362115
* 何李. (2022). 语义网络的中枢概念对创造性思维的影响及其认知神经机制 [博士, 西南大学]. https://doi.org/10.27684/d.cnki.gxndx.2022.000016


## 网络分析的基础知识

* https://dshizuka.github.io/networkanalysis/04_measuring.html


### 节点（nodes）

1. 使用什么样的语言特征来作为节点量化特征？
    * 显示特征
    * 隐形特征——word embeddings
2. 使用哪些词汇作为节点？
    * 先验方法，事先选取感兴趣的关键词，例如多人独立根据任务要求选取，再合并去重
    * 后验方法，通过概率选取高频词等基于数据驱动的方法
3. 使用多少个词汇作为节点？
  * 节点多，则网络密集（dense）；节点少，则网络稀疏
  * 研究表明，相比稀疏网络，密集网络中相关结果更好；https://www.nature.com/articles/srep08665



### 边：语义相似性和语义距离

* 从信息论的角度来看，相似性被定义为两个文本片段之间的共同点。
* Wang, J., & Dong, Y. (2020). Measurement of text similarity: a survey. Information, 11(9), 421.
* **语义相关性**包括两个术语之间的任何关系，而**语义相似性**仅包括“是”关系。 例如，“car”与“bus”类似，但也与“road”、“driving”相关。

### 相似的程度

* 语义相似度是有程度大小之分的。
* 粒度是指数据仓库的数据单位中保存数据的细化或综合程度的级别。细化程度越高，粒度级就越小；相反，细化程度越低，粒度级就越大。在数据仓库环境中,粒度是一个重要的设计问题,他影响到数据仓库的数据量和系统能回答的查询类型,粒度越小,细节程度越高,能回答查询就越多,但是存储的东西也就越多。
* Demo
    * 相似句：肺部发炎是什么原因引起的-肺部发炎是什么引起的
    * 不相似句：肺部发炎是什么原因引起的-肺部炎症有什么症状
* 什么叫做相似，什么叫做不相似，依赖于场景和任务的具体要求是什么。以Demo来看：
    * 如果是判断大粒度意图的话（是否是医疗问句）那就是相似句
    * 如果是小粒度（症状意图、病因意图）那这两句就不相似。

### 相关理论

* 有什么理论来可以解释语义的相似性？
* 有什么理论支持语义网络的构建？
* 语义网络分析的优势和劣势都是什么？

### 语义相似度计算方法

1. 余弦相似度（Cosine similarity） (Landauer et al., 1998)
2. 欧式距离（Euclidean Distance）
3. 曼哈顿距离（Manhattan Distance）
4. 明可夫斯基距离（Minkowski distance）

* 韩程程, 李磊, 刘婷婷, 高明. 语义文本相似度计算方法[J]. 华东师范大学学报（自然科学版）, 2020, (5): 95-112. doi: 10.3969/j.issn.1000-5641.202091011

### GPT 答疑

* **现在我要用一些词汇的word embeddings来构建语义网络。我计算了这些词汇word embeddings之间的余弦相似度（cosine similarity）。我现在不确定，这个语义网络中，节点之间 边是否应该对应余弦相似度。或者我需要先把用1减去余弦相似度获得余弦距离，用余弦距离来做反应语义网络的边。请你根据语义网络的前人研究，分析一下我应该用余弦相似度还是余弦距离作为语义网络的边，边的长短应该怎么解释，给出具体的依据和参考文献。**
    * 在构建语义网络时，可以使用余弦相似度或余弦距离作为节点之间边的权重，具体取决于你希望边表示的含义。
    * 如果你希望边表示两个词汇的语义接近程度，那么可以使用余弦相似度。余弦相似度的值范围为-1到1，其中1表示完全相同，0表示无关，-1表示完全相反。这种方式下，边的长度可以解释为语义的差异程度：长度越短，语义越相似；长度越长，语义越不同。
    * 另一方面，如果你希望边表示两个词汇的语义距离，那么可以使用1减去余弦相似度得到的余弦距离。这样，边的长度就直接对应了语义的距离：长度越短，距离越近，即语义越相似；长度越长，距离越远，即语义越不同。

* 以下是一些相关参考文献：
      * Kenett, Y. N., Levi, E., Anaki, D., & Faust, M. (2017). The semantic distance task: Quantifying semantic distance with semantic network path length. Journal of Experimental Psychology: Learning, Memory, and Cognition, 43(9), 1470–1489. https://doi.org/10.1037/xlm0000391
      * Hashemikamangar, S. S., Bakouie, F., & Gharibzadeh, S. (2020, November). Children Semantic Network Growth: A Graph Theory Analysis. In 2020 27th National and 5th International Iranian Conference on Biomedical Engineering (ICBME) (pp. 318-321). IEEE.
      * Nematzadeh, A., Fazly, A., & Stevenson, S. (2014, October). A cognitive model of semantic network learning. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 244-254).
      * Harrington, B. (2010, August). A semantic network approach to measuring relatedness. In Coling 2010: Posters (pp. 356-364).

* **为什么1减去余弦相似度可以得到余弦距离？为什么余弦距离可以反映语义距离**
    * 余弦相似度是衡量两个向量方向相似度的一种方法，其值的范围在-1到1之间。当两个向量完全重合时，余弦相似度为1；当两个向量垂直时，余弦相似度为0；当两个向量方向完全相反时，余弦相似度为-1。因此，余弦相似度越接近1，表示两个向量（在这里即词汇的word embeddings）越相似。
    * 然而，在很多情况下，我们更关心的是两个向量之间的“距离”或者说“差异”，而非它们的相似度。这时候，我们可以通过计算“1减去余弦相似度”来得到一个新的指标，也就是所谓的“余弦距离”。这样做的好处是，当两个向量完全重合（即完全相似）时，余弦距离为0；当两个向量完全不相关时，余弦距离为1。换句话说，“余弦距离”能够很好地反映出两个向量之间的差异程度。
    * 在语义分析中，每个词汇都可以通过word embeddings转化为一个高维空间中的点（或者说向量）。因此，我们可以通过计算不同词汇之间word embeddings的“余弦距离”，来衡量这些词汇在语义上的“差异”或者说“距离”。

* 参考文献：
  - Singh, P., & Sawhney, R. (2018). Semantic similarity in WordNet and Word2Vec for cyberbullying detection. 2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI). IEEE.
  - Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).


## 可视化

* 我们可以使用Wordij自带的可视化（Visij）功能来进行语义网的可视化。
* 如果追求进一步的聚类分析（Cluster Analysis）与更加美观的可视化效果，则可以将Wordij的输出结果进一步地输入到Gepihi软件(https://gephi.org/)中，利用Gephi来实现语义网络的可视化。具体方法为：
    * 将Wordji中生成的Net文件
    * 将net文件导入Gephi中，并利用Gephi的模块化功能实现节点的聚类，或利用度筛选功能实现节点筛选等。

## Paper1

* Miani, A., Hills, T., & Bangerter, A. (2022). Interconnectedness and (in) coherence as a signature of conspiracy worldviews. Science Advances, 8(43), eabq3668.

### 研究思路

* 作者的核心思路在于，如何把阴谋论文本网络化，网络的节点和连接应该对应什么概念，如何量化和计算？
* 理解该文章思路的关键在于搞清楚，作者如何利用种子点和主题词的共现（the co-occurrences of seeds and LDA topics）来进行语义网络的构建。
* 语义网络
    * 节点（nodes）： seeds vs. topics
    * 边（edges）：the co-occurrences of seeds and LDA topics。
    * 数据格式：行（rows）是文件/报告（documents） * 列是seeds or topics。例如，有100篇报道，每篇报道有10-20个主题。
    * 关键的操作，如何从topics相关矩阵，转换得到co-occurrences矩阵。
* topics和seeds的向量化。此处应当了解，类似于word embeddings的概念，每个topic或seed在数据中都应该进行了向量化。然后，不同的topics之间就可以进行perason相关计算。
例如吴孟达和周星驰之间的相关，就应该高于梦露和周星驰之间的相关。
* 背景知识：在LDA主题模型中，每个主题都是一个概率分布，表示该主题下单词出现的概率。这个概率分布可以看作是一个向量，其中每个元素表示该单词在该主题下的概率。例如，假设有一个主题关于体育，它的关键词分布可能是：“篮球：0.2，足球：0.1，棒球：0.05，跑步：0.03，运动：0.02”等。
同样地，种子词也可以表示成向量形式。例如，假设有3个种子词：“健康”、“锻炼”和“饮食”，它们可以表示为一个3维向量：“健康：[0.5, 0.3, 0.2], 锻炼:[0.2, 0.3, 0.5], 饮食:[0.1, 0.2, 0.7]”。这种向量的表示方式有助于在主题模型中进行数学运算和统计推理。通过向量运算，可以计算单词之间的相似性、主题之间的相关性等。
* 为了解释如何从between-topic correlation matrix转换到co-occurrence matrices，我们可以考虑以下步骤。
    1. 首先，假设我们已经得到了一个between-topic correlation matrix，其中每个元素表示一对主题之间的相关性。
    2. 然后，我们可以将这个matrix扩展到包含所有主题对的相关性。在这个扩展matrix中，每行表示一个主题与其他所有主题的相关性。
    3. 接下来，我们可以将这个扩展matrix转换为co-occurrence matrices。具体来说，我们可以为每个文档创建一个co-occurrence matrix，其中每个元素表示一对关键词在该文档中的共现频率。
    4. 最后，我们可以使用这些co-occurrence matrices来构建共现网络，其中每个节点表示一个关键词，每条边表示一对关键词之间的共现关系。
以下是一个简单的例子来说明这个过程：

假设我们有一个包含三个主题A、B和C的between-topic correlation matrix：

```css
| A | B | C |
|---|---|---|
| 0.8 | 0.2 | 0.1 |
| 0.2 | 0.8 | 0.3 |
| 0.1 | 0.3 | 0.8 |
```
然后，我们将这个matrix扩展到包含所有主题对的相关性：

```css
| A | B | C | AB | AC | BC | ABC |
|---|---|---|----|----|----|-----|
| A | 0.8 | 0.2 | 0.8 | 0.2 | 0.2 | 0.8 |
| B | 0.2 | 0.8 | 0.2 | 0.8 | 0.3 | 0.3 |
| C | 0.1 | 0.3 | 0.1 | 0.3 | 0.3 | 0.1 |
```
接下来，我们为每个文档创建一个co-occurrence matrix：

假设我们有一个文档d，其中包含了主题A、B和C的关键词，并且这些关键词在文档中以以下顺序出现：A -> B -> C -> A -> B -> C -> A。

那么，对应的co-occurrence matrix如下：

```css
| A | B | C | AB | AC | BC | ABC |
|---|---|---|----|----|----|-----|
| 1 | 1 | 1 | 1 | 1 | 1 | 1 | A -> B -> C -> A -> B -> C -> A
```
最后，我们可以使用这些co-occurrence matrices来构建共现网络。在这个例子中，我们只有一个co-occurrence matrix，所以我们可以直接使用该matrix构建一个包含三个节点的共现网络。

### 研究方法探讨

* 作者使用R包`topicmodels`来进行主题提取。以下是一个简单的例子，说明如何使用该包进行主题提取：

```R
# 安装和加载topicmodels包
install.packages("topicmodels")
library(topicmodels)

# 加载LOCO语料库
data("loco")

# 对语料库进行预处理
preproc <- function(words) {
  words <- tolower(words)
  words <- tm_map(words, content_transformer(removePunctuation))
  words <- tm_map(words, content_transformer(removeNumbers))
  words <- tm_map(words, content_transformer(removeWords), stopwords("smart"))
  words <- tm_map(words, content_transformer(stripWhitespace))
  return(words)
}
loco <- preproc(loco)

# 创建语料库矩阵
matrix <- TermDocumentMatrix(loco)

# 使用LDA算法进行主题提取
lda_model <- LDA(matrix, k = 5) # 这里选择5个主题

# 输出主题信息
print(lda_model)

# 输出每个主题中概率较高的单词
topics <- topics(lda_model)
```
在上述代码中，首先加载了`topicmodels`包，然后加载了LOCO语料库。接下来，定义了一个预处理函数`preproc`，用于对语料库中的文本进行预处理，包括转换为小写、去除标点符号、去除数字、去除停用词和去除空白。然后，对LOCO语料库进行预处理。
接下来，创建了一个语料库矩阵，然后使用`LDA()`函数进行主题提取。在`LDA()`函数中，指定了矩阵和主题数量（k值）。然后，通过打印模型信息可以查看主题信息。
最后，使用`topics()`函数提取每个主题中概率较高的单词，这将给出每个主题的主题分布。

* 在该文中，Seeds和LDA topics有和区别和联系，请举例说明
在文中，Seeds和LDA topics是两个不同的概念。Seeds是指作者在LOCO语料库中选取的一组关键词，用于初始化LDA模型的主题分布。而LDA topics则是通过运行LDA模型后从语料库中提取出来的主题。
Seeds是一组具有代表性的单词，它们在LOCO语料库中出现频率较高，可以代表该语料库中的重要信息。作者选取了35个Seeds，并将其分为五组，每组七个单词。Seeds的作用是提供主题分布的初始值，以加速LDA模型的收敛。
LDA是一种基于概率的主题模型，它通过对文本中的词频和词序进行分析，推断出文本的主题。在运行LDA模型后，从LOCO语料库中提取出了五个主题。这些主题涵盖了不同的领域和话题，例如第一主题涉及医疗、保健和疾病，第二主题涉及社会、文化和媒体，第三主题涉及政治、法律和权力，第四主题涉及经济、商业和市场，第五主题涉及环境、生态和自然。
虽然Seeds只是用于初始化LDA模型的主题分布，但Seeds的选择对于主题的提取具有一定的影响。例如，如果Seeds的选取不具有代表性或过于局限于某一领域，可能会导致主题分布的不均衡或偏差。因此，Seeds的选择应该考虑到语料库的多样性和代表性。
总之，Seeds和LDA topics是不同的概念，但Seeds的选择对于LDA模型的运行和主题提取具有一定的指导作用。
* 根你的回答我，我感觉作者应该先确定seeds，在根据seeds确定topics。但是实际上，作者先进行了topic extraction，再进行了seeds extraction。在topic extraction的过程中，似乎并没有用到seeds？
你是对的，作者在文章中的步骤是先进行了主题提取(topic extraction)，然后再进行了种子词提取(seeds extraction)。在这个过程中，确实没有直接使用Seeds来提取Topics。
实际上，Seeds主要用于初始化LDA模型的主题分布，这对模型的结果会有一定影响。但并不是说必须要先确定Seeds，再提取Topics。你也可以直接对整个语料库运行LDA模型，而不需要事先确定Seeds。
作者可能选择先进行主题提取，然后再提取种子词，是为了确保提取的Topics能尽可能地反映语料库的真实主题，而不是被Seeds所引导。因为Seeds虽然可以提供初始的主题分布，但如果Seeds的选择不够准确，可能会影响到最终的主题提取结果。
总的来说，虽然Seeds和Topics有联系，但它们是两个不同的步骤，而且它们的提取顺序并不是固定的，可以根据实际需求进行调整。
* 我感觉作者是将seeds与topics视为两种并行的关系，前者是基于先验概率的关键词，后者是基于后验概率的关键词。作者后面同时进行了基于seeds和基于topics的网络构建，这可能是为了避免方法偏倚，尽可能保证所构建的语义网络反映了实际的文本情况。
* 通过LDA主题提取，构建共现网络——co-occurrences of LDA topics是什么意思，请举例说明
"Co-occurrences of LDA topics"指的是LDA主题的共现。LDA（Latent Dirichlet Allocation）是一种主题模型，用于从大量文本中提取主题。在LDA中，每个主题都由一组概率分布的关键词构成。当一个文档涵盖了多个主题时，这些主题的关键词可能会在该文档中同时出现，这就形成了主题的共现。
举例来说，假设我们有一个关于电影的语料库，我们使用LDA从中提取了三个主题，分别是"科幻"、"动作"和"喜剧"。在某个文档中，如果同时出现了"星球大战"、"詹姆斯·邦德"和"周星驰"这三个关键词，那么我们就可以说这三个主题（"科幻"、"动作"和"喜剧"）在该文档中发生了共现。
Co-occurrences of LDA topics可以帮助我们理解不同主题之间的联系和相互影响。通过分析主题共现的情况，我们可以发现某些主题之间可能存在的关联或者排斥关系，这对于理解文本的整体结构和语义内涵具有重要的意义。

* 作者具体如何使用igraph构建共现网络，举例说明
作者具体使用igraph构建共现网络的方法如下：

1. 安装和加载igraph包：首先，安装igraph包，然后加载igraph包。


```R
install.packages("igraph")
library(igraph)
```
2. 构建共现关系矩阵：使用上一步中的共现关系矩阵构建igraph对象。


```R
cooccurrence_matrix <- matrix(cooccurrences, nrow = length(seeds), ncol = length(seeds))
ig_graph <- graph_from_adjacency_matrix(cooccurrence_matrix, mode = "undirected", weighted = TRUE)
```
这里，`cooccurrences`是关键词的共现频率矩阵，`seeds`是种子词列表。`graph_from_adjacency_matrix()`函数根据共现矩阵创建一个无向图，其中每个节点对应一个种子词，每个边对应两个种子词之间的共现关系。`mode = "undirected"`表示创建无向图，`weighted = TRUE`表示边的权重是基于共现频率的。

3. 可视化共现网络：使用igraph的函数对共现网络进行可视化。


```R
plot(ig_graph, vertex.size=10, vertex.label.cex=0.8)
```
这里，`plot()`函数用于绘制图形，`vertex.size=10`表示节点的大小为10，`vertex.label.cex=0.8`表示节点标签的字体大小为0.8倍。

举例来说，假设我们有一个关于电影的语料库，我们使用LDA从中提取了三个主题，分别是"科幻"、"动作"和"喜剧"。假设种子词列表为`seeds = c("星球大战", "詹姆斯·邦德", "周星驰")`，关键词的共现频率矩阵为：


```csharp
cooccurrences = array(c(30, 20, 10, 15, 25, 10, 5, 5, 5, 5), dim = c(3, 4))
```
那么，我们可以按照上述步骤构建一个igraph对象，并使用`plot()`函数对其进行可视化。结果将显示一个包含三个节点的无向图，节点对应于种子词"星球大战"、"詹姆斯·邦德"和"周星驰"，边对应于这些种子词之间的共现关系。边的权重将根据共现频率进行赋值。

## Paper2 大脑中语义网络的构建

* 最近，Huth等人(2012;2016)开发了一种方法，通过使用自然语言处理技术对功能磁共振成像（fMRI）大脑活动进行建模，可以定量评估大脑中单个单词的语义表示（即，大脑单词嵌入，word embeddings in the brain）。该方法启发我们通过评估大脑词嵌入之间的相似性来表征人脑中的语义网络(Schematic Network)，并使用网络分析分析其结构特征。
* Matsuto等人（2023）使用上述方法，发现精神分裂症个体的语义网络相比正常个体，不具有小世界属性，呈现出模块化的特征，即结构更加混乱。
* Huth AG, Nishimoto S, Vu AT, Gallant JL. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron. 2012;76(6):1210–1224.
*	Huth AG, De Heer WA, Griffiths TL, Theunissen FE, Gallant JL. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature. 2016;532(7600):453–458.
* Matsumoto, Y., Nishida, S., Hayashi, R., Son, S., Murakami, A., Yoshikawa, N., Ito, H., Oishi, N., Masuda, N., Murai, T., Friston, K., Nishimoto, S., & Takahashi, H. (2023). Disorganization of Semantic Brain Networks in Schizophrenia Revealed by fMRI.Schizophrenia bulletin, 49(2), 498–506. https://doi.org/10.1093/schbul/sbac157
* 国内报道：https://mp.weixin.qq.com/s/etKNlDcQvxhWCttglNrVcA

### 研究思路

* 整体的思路如下：
    1. 将语言任务中的高频词，通过word2vec向量化，得到感兴趣语料的向量矩阵；
    2. fMRI多体素模式分析：将电影场景引起的神经活动的多体素模式建模为场景向量矩阵和模型权重的矩阵乘积
    3. 利用体素编码模型的权重对每位受试者的大脑词嵌入进行量化。首先，选取预测准确率最高的前 7000 个体素来创建权重矩阵。然后，将电影场景描述中出现频率最高的 11132 个单词中的前 1000 个单词在 word2vec 向量空间中的原始单词向量乘以模型权重，得到每个单词在大脑中的向量表示。

# 2. 传统词频分析和进阶版本（DDR）

## 背景知识：道德基础理论（Moral Foundations Theory，MFT）

* MFT的核心是试图解释不同人的道德一致性的整体性。具体来说，MFT试图阐明政治保守派和自由派道德之间的差异（Graham et al.， 2011）。这些差异是通过衡量个人对五个道德基础的认可来建立的。
    * 前两个基础：伤害/关怀和公平/互惠——代表了对以个人为中心的社会正义和平等的关注。这两个基础可以概念化为个性化基础。
    * 后三个基础：群体内/忠诚，权威/尊重和纯洁/神圣 - 代表了从群体层面对是非的看法，可以被认为是具有约束力的基础（Haidt&Graham，2007）。
    * 研究（如下所述）表明，政治自由主义者倾向于支持这两个个性化的基础，而保守派则倾向于支持所有五个基础，而对约束性基础的认可更大（Graham，Haidt，&Nosek，2009）。
* Graham等人（2009）开发了道德基础词典（Moral Foundations Dictionary，MFD），以确定语音和文本中对五个基础的认可。MFD 由每个基础大约 50 个单词组成，以举例说明其含义。例如，诸如“虐待”（abuse）和“保护”之类的词表示对伤害/关怀基础的认可。Graham et al. （2009） 然后通过分析自由派和保守派教会布道的语音内容来验证 MFD 词典列表。他们发现自由派的长对话（sermons）支持个人化的基础，而保守派的长对话（sermons）支持所有五个道德基础。

## 传统词频分析

* 词频分析可以将定性数据（例如新闻文章）转化为可衡量的结果。通过词频分析，我们可以量化不同来源的政治语言（political language ），从而可以检验不同主题的心理学问题。
* 搜集文本（collections），清洗文本（clean），获得词汇。
* 同时对搜集到的文本数据、以及词典中的词汇进行词干提取（stemming）。每个概念或单词都有多种形式，例如走路、走路和走路。 词干提取指从收集的文本数据中去除无用的词缀，以便能够将具有相似含义的单词组合起来进行分析。例如使用了 Snowball 词干分析器下的英语选项进行词干提取。
* 然后将每篇文章中使用的单词与字典中的单词进行比较，并从更大的集合中只选择那些单词。例如，"equal"（平等）被纳入 "fairness"（公平）的总词汇中，而 "another"、"the "和 "over "等其他词汇则被忽略。这样，我们就可以获得每个类型中（如“公平”），数据文本中该类词汇的频数。使用频数除以原始文本总字数，即获得其词频。
* 数据可视化。

## 分布式词典表征（distributed dictionary representations，DDR）

* 研究目的：评估民主党人和共和党人在多大程度上使用了与道德基础理论提出的领域（伤害、公平、忠诚、权威和纯洁）相关的语言，以及这些语言如何随着政治权力的变化而变化。
* Wang等人（2021）的方法，基于词频分析技术，结合分布式词典表征技术，即将数据文本中的单词以及词典中的单词都转换为词嵌入，计算两者之间的余弦相似性（-1到1）；用余弦相似性来表示。而不是直接计算词频。**问题，文本词汇与词典词汇之间的相似性达到多大，可以认为存在归属关系？**
* Garten 等人 （2018） 开发了 ddr Python 模块来计算数据文本（如推特推文）和特定简短词典（如代表每个道德基础的积极和消极方面（“美德”和“恶习”）的MFD）之间的语义相似性。
* 例如，包含“car”的文本可能被认为与由“automobile”、“auto”和“vehicle”组成的字典高度相似。这意味着词典可以很短，但仍然很好地涵盖了一个概念；文本可以很短，但仍被认为类似于字典。这消除了字数统计方法的两个主要缺点（由于词典范围较小或文本较短而漏保；由于词典范围太长或匹配问题而误报，如“happ*” matches “happy” and “happiness” but also “happened” and “happenstance”）。在将自动化方法与人类编码人员进行比较的验证研究中，这种方法比字数统计准确得多（Garten 等人，2018 年）。
* Wang等人（2021）的自变量是党派（自由 vs. 共和），因变量是每条推文的10个相似性值。对与每条推文，5个道德基础*积极和效价=10个相似值。
* 将自变量设置为1个虚拟变量（1 = 民主党，0 = 共和党）。分别针对10个指标建立了10个回归模型。结果发现：在每一个基础和方面，民主党人都比共和党人更多地使用道德语言，所有 ps < 0.001（见图 1）。

## 参考文献

* 传统词频分析教程：https://methods.sagepub.com/dataset/word-frequency-analyse-political-language-moral
* Graham, J., Nosek, B. A., Haidt, J., Iyer, R., Koleva, S., & Ditto, P. H. (2011). Mapping the moral domain. Journal of Personality and Social Psychology, 101(2), 366–385. https://doi.org/10.1037/a0021847
* Graham, J., Haidt, J., & Nosek, B. A. (2009). Liberals and conservatives rely on different sets of moral foundations. Journal of Personality and Social Psychology, 96(5), 1029–1046. https://doi.org/10.1037/a0015141
* Wang, S. Y. N., & Inbar, Y. (2021). Moral-language use by US political elites. Psychological Science, 32(1), 14-26.
* Garten J., Hoover J., Johnson K. M., Boghrati R., Iskiwitch C., Dehghani M. (2018). Dictionaries and distributions: Combining expert knowledge and large scale textual data content analysis. Behavior Research Methods, 50, 344–361.
* Padfield, W. E., & Buchanan, E. M. (2020). Moral foundations of US political news organizations.
